{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import login\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"HF_TOKEN\") # Replace HF_TOKEN with your secret name\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import os\n",
    "import wandb\n",
    "%wandb login 46c9dd6a8552e990c5901e2d4c0425f5b9e11232\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"yiyanghkust/finbert-tone\"\n",
    "DATA_PATH = \"/kaggle/working/stock-market-predictive-analysis/data/news_sentiment/labeled_news.csv\"  # path to your CSV\n",
    "OUTPUT_DIR = \"./finbert_finetuned\"\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "LR = 1e-5\n",
    "MAX_LEN = 128\n",
    "PROJECT_NAME = \"finbert-finetune\"  # wandb project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.init(project=PROJECT_NAME, name=\"finbert-local-kaggle\", config={\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": LR,\n",
    "    \"max_length\": MAX_LEN\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.init(project=PROJECT_NAME, name=\"finbert-local-kaggle\", config={\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": LR,\n",
    "    \"max_length\": MAX_LEN\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "df = df.dropna(subset=[\"News\", \"PrevClose_Label\"])\n",
    "\n",
    "# Label mapping\n",
    "label_map = {label: i for i, label in enumerate(df[\"PrevClose_Label\"].unique())}\n",
    "num_labels = len(label_map)\n",
    "df[\"label\"] = df[\"PrevClose_Label\"].map(label_map)\n",
    "\n",
    "# Train/test split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"News\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "test_ds = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "# Remove unused columns\n",
    "remove_cols = [\"Company\", \"Date\", \"News\", \"Intraday_Label\", \"Intraday_Change(%)\"]\n",
    "train_ds = train_ds.remove_columns([col for col in remove_cols if col in train_ds.column_names])\n",
    "test_ds = test_ds.remove_columns([col for col in remove_cols if col in test_ds.column_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    report_to=\"none\",\n",
    "    run_name=\"finbert-local-kaggle\",  # appears in wandb dashboard\n",
    "    load_best_model_at_end=True,  # keeps best checkpoint by eval metric\n",
    "    metric_for_best_model=\"f1\",   # use F1 to pick the best\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    # Identify the label ID for \"neutral\" (case-insensitive)\n",
    "    neutral_label_id = None\n",
    "    for key in label_map.keys():\n",
    "        if key.lower() == \"neutral\":\n",
    "            neutral_label_id = label_map[key]\n",
    "            break\n",
    "    \n",
    "    if neutral_label_id is not None:\n",
    "        # Ignore neutral predictions and ground truths\n",
    "        mask = (labels != neutral_label_id) & (preds != neutral_label_id)\n",
    "        # mask = labels != neutral_label_id\n",
    "\n",
    "        if np.sum(mask) == 0:\n",
    "            return {\"accuracy\": 1.0, \"f1\": 1.0}\n",
    "        preds = preds[mask]\n",
    "        labels = labels[mask]\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T21:12:07.129450Z",
     "iopub.status.busy": "2025-10-21T21:12:07.128925Z",
     "iopub.status.idle": "2025-10-21T21:15:08.732182Z",
     "shell.execute_reply": "2025-10-21T21:15:08.731527Z",
     "shell.execute_reply.started": "2025-10-21T21:12:07.129424Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">finbert-local-kaggle</strong> at: <a href='https://wandb.ai/eshaan-rithesh2023-vit-chennai/finbert-finetune/runs/o3qqbjl7' target=\"_blank\">https://wandb.ai/eshaan-rithesh2023-vit-chennai/finbert-finetune/runs/o3qqbjl7</a><br> View project at: <a href='https://wandb.ai/eshaan-rithesh2023-vit-chennai/finbert-finetune' target=\"_blank\">https://wandb.ai/eshaan-rithesh2023-vit-chennai/finbert-finetune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_211137-o3qqbjl7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20251021_211208-lkcsnpue</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eshaan-rithesh2023-vit-chennai/finbert-finetune/runs/lkcsnpue' target=\"_blank\">finbert-local-kaggle</a></strong> to <a href='https://wandb.ai/eshaan-rithesh2023-vit-chennai/finbert-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eshaan-rithesh2023-vit-chennai/finbert-finetune' target=\"_blank\">https://wandb.ai/eshaan-rithesh2023-vit-chennai/finbert-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eshaan-rithesh2023-vit-chennai/finbert-finetune/runs/lkcsnpue' target=\"_blank\">https://wandb.ai/eshaan-rithesh2023-vit-chennai/finbert-finetune/runs/lkcsnpue</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25b18ec1a8d4a3bb4c9f3b1fa3bf1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afe212fb0044a999f214df525c8d7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/188 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2749/4062237228.py:115: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='470' max='470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [470/470 02:49, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.033388</td>\n",
       "      <td>0.717105</td>\n",
       "      <td>0.616281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.194500</td>\n",
       "      <td>0.992197</td>\n",
       "      <td>0.739437</td>\n",
       "      <td>0.721963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.884300</td>\n",
       "      <td>1.014713</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>0.701394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.741800</td>\n",
       "      <td>1.113563</td>\n",
       "      <td>0.744828</td>\n",
       "      <td>0.721099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.633000</td>\n",
       "      <td>1.153524</td>\n",
       "      <td>0.715278</td>\n",
       "      <td>0.709407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>1.189672</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.701603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.415900</td>\n",
       "      <td>1.265786</td>\n",
       "      <td>0.696552</td>\n",
       "      <td>0.683073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.319100</td>\n",
       "      <td>1.284147</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.719016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>1.319678</td>\n",
       "      <td>0.710145</td>\n",
       "      <td>0.701538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.258700</td>\n",
       "      <td>1.333564</td>\n",
       "      <td>0.707143</td>\n",
       "      <td>0.697611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">finbert-local-kaggle</strong> at: <a href='https://wandb.ai/eshaan-rithesh2023-vit-chennai/finbert-finetune/runs/lkcsnpue' target=\"_blank\">https://wandb.ai/eshaan-rithesh2023-vit-chennai/finbert-finetune/runs/lkcsnpue</a><br> View project at: <a href='https://wandb.ai/eshaan-rithesh2023-vit-chennai/finbert-finetune' target=\"_blank\">https://wandb.ai/eshaan-rithesh2023-vit-chennai/finbert-finetune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_211208-lkcsnpue/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Model saved to: ./finbert_finetuned\n",
      "Label mapping: {'Negative': 0, 'Positive': 1, 'Neutral': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9921973943710327,\n",
       " 'eval_accuracy': 0.7394366197183099,\n",
       " 'eval_f1': 0.7219629767047157,\n",
       " 'eval_runtime': 1.1332,\n",
       " 'eval_samples_per_second': 165.907,\n",
       " 'eval_steps_per_second': 10.59,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------- FINBERT FINE-TUNING (KAGGLE LOCAL + WANDB) --------------------\n",
    "# ---------------- CONFIG ----------------\n",
    "# ----------------------------------------\n",
    "# ---------------- INIT WANDB ----------------\n",
    "# ---------------- LOAD DATA ----------------\n",
    "# ---------------- TOKENIZER ----------------\n",
    "# ---------------- TRAINING SETUP ----------------\n",
    "# ---------------- METRICS ----------------\n",
    "# ---------------- TRAINER ----------------\n",
    "# ---------------- TRAIN ----------------\n",
    "trainer.train()\n",
    "# ---------------- SAVE MODEL ----------------\n",
    "\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "print(\"Training complete. Model saved to:\", OUTPUT_DIR)\n",
    "print(\"Label mapping:\", label_map)\n",
    "\n",
    "trainer.evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8515963,
     "sourceId": 13417573,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
